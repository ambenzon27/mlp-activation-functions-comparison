# ðŸ”¬ MLP Activation Function Benchmarking

This project benchmarks the performance of three activation functionsâ€”**Logistic (Sigmoid)**, **Tanh**, and **Leaky ReLU**â€”in a two-layer Multilayer Perceptron (MLP) trained via backpropagation with momentum. It evaluates how each function impacts convergence speed and classification performance on a multi-class dataset.

