# ðŸ” MLP Activation Function Benchmarking

This project benchmarks the effect of different activation functionsâ€”**Logistic**, **Tanh**, and **Leaky ReLU**â€”on the training performance of a Multilayer Perceptron (MLP) using backpropagation with momentum. Designed for educational and research purposes, it compares training efficiency and classification quality across activation types using precision metrics like **Accuracy**, **F1 Score**, and **Matthews Correlation Coefficient (MCC)**.

---

## ðŸš€ Project Highlights

- ðŸ”§ Two-layer MLP with user-configurable activations
- ðŸ“ˆ Tracks Accuracy, F1 Score, and MCC over epochs
- ðŸ§  Implements gradient descent with momentum
- ðŸ§ª Mini-batch training with learning curve visualization
- ðŸ—ƒï¸ Balanced synthetic or real multi-class dataset support
- ðŸ“Š Activation-wise performance summary and plots

---

## ðŸ“ Repository Structure
mlp-activation-functions-comparison/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data.csv
â”‚   â””â”€â”€ data_labels.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_mlp.py                # main training runner (all activations)
â”‚   â”œâ”€â”€ model.py                    # MLP architecture + activation selection
â”‚   â”œâ”€â”€ backpropagation.py         # forward, backward, update weights
â”‚   â”œâ”€â”€ metrics.py                 # accuracy, precision, recall, F1, MCC
â”‚   â”œâ”€â”€ data_loader.py             # read CSVs, normalize, split data
â”‚   â”œâ”€â”€ activation_functions.py    # tanh, logistic, leaky ReLU definitions
â”‚   â”œâ”€â”€ plotter.py                 # learning curves, confusion matrix, etc.
â”‚   â””â”€â”€ config.py                  # hyperparameters and options
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ logistic/
â”‚   â”‚   â”œâ”€â”€ results.json
â”‚   â”‚   â”œâ”€â”€ loss_curve.png
â”‚   â”‚   â””â”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ tanh/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ leaky_relu/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ summary_comparison.png
â”‚   â”œâ”€â”€ activation_table.md
â”‚   â””â”€â”€ architecture_diagram.png
â”‚
â””â”€â”€ notebook/
    â””â”€â”€ mlp_activation_benchmark.ipynb 

---

## ðŸ“Š Example Results

| Activation   | Accuracy | F1 Score | MCC    |
|--------------|----------|----------|--------|
| Logistic     | 0.88     | 0.87     | 0.84   |
| Tanh         | 0.90     | 0.89     | 0.87   |
| Leaky ReLU   | 0.93     | 0.92     | 0.90   |

> ðŸ“Œ *Values are illustrative. See `/figures/` for real plots and results.*

---

## ðŸ§° Setup Instructions

### âœ… Install Dependencies
```bash
pip install ------

Prepare the Data
Put your data in the data/ folder:

data.csv: shape = [n_samples, n_features]

data_labels.csv: shape = [n_samples, 1]

Don't have data? Generate synthetic data:
python src/utils.py --generate-data


Train an MLP with a specific activation function:
python src/train_mlp.py --activation tanh


Supported activations:

- logistic
- tanh
- leakyrelu



