# src/config.py
CONFIG = {
    "hidden_layers": [128, 64],
    "activation": "tanh",  # Options: "logistic", "tanh", "leaky_relu"
    "epochs": 150,
    "batch_size": 8,
    "lr": 0.001,
    "momentum": 0.9,
    "random_seed": 42
}

# src/activation_functions.py
import numpy as np

def logistic(x):
    return 1 / (1 + np.exp(-x))

def logistic_derivative(x):
    sig = logistic(x)
    return sig * (1 - sig)

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2

def leaky_relu(x):
    return np.where(x > 0, x, 0.01 * x)

def leaky_relu_derivative(x):
    return np.where(x > 0, 1, 0.01)

# src/model.py
import numpy as np
from activation_functions import logistic, tanh, leaky_relu

ACT_FUNCS = {
    "logistic": logistic,
    "tanh": tanh,
    "leaky_relu": leaky_relu
}

def initialize_weights(layer_dims):
    weights = {}
    for i in range(1, len(layer_dims)):
        weights[f"W{i}"] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01
        weights[f"b{i}"] = np.zeros((layer_dims[i], 1))
    return weights

def forward_propagation(X, weights, activations):
    A = X.T
    cache = {"A0": A}
    L = len(activations)
    for l in range(1, L+1):
        Z = weights[f"W{l}"].dot(A) + weights[f"b{l}"]
        A = ACT_FUNCS[activations[l-1]](Z)
        cache[f"Z{l}"] = Z
        cache[f"A{l}"] = A
    return A, cache

# src/backpropagation.py
import numpy as np
from activation_functions import logistic_derivative, tanh_derivative, leaky_relu_derivative

DER_FUNCS = {
    "logistic": logistic_derivative,
    "tanh": tanh_derivative,
    "leaky_relu": leaky_relu_derivative
}

def compute_loss(Y_hat, Y):
    m = Y.shape[0]
    return -np.sum(Y.T * np.log(Y_hat + 1e-8)) / m

def backward_propagation(X, Y, weights, cache, activations):
    grads = {}
    m = X.shape[0]
    L = len(activations)
    Y = Y.T
    dA = cache[f"A{L}"] - Y

    for l in reversed(range(1, L + 1)):
        dZ = dA * DER_FUNCS[activations[l-1]](cache[f"Z{l}"])
        dW = (1 / m) * dZ.dot(cache[f"A{l-1}"].T)
        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)
        dA = weights[f"W{l}"].T.dot(dZ)
        grads[f"dW{l}"] = dW
        grads[f"db{l}"] = db
    return grads

def update_weights(weights, grads, lr, momentum, v):
    for key in weights.keys():
        grad_key = f"d{key}"
        v[key] = momentum * v[key] - lr * grads[grad_key]
        weights[key] += v[key]
    return weights, v

# src/data_loader.py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

def load_data(data_path, labels_path, test_size=0.2):
    X = pd.read_csv(data_path).values
    y = pd.read_csv(labels_path).values.ravel()

    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    enc = OneHotEncoder(sparse=False)
    y_onehot = enc.fit_transform(y.reshape(-1, 1))

    return train_test_split(X_scaled, y_onehot, test_size=test_size, random_state=42)

# src/metrics.py
from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef
import numpy as np

def evaluate(y_true, y_pred):
    y_true_class = np.argmax(y_true, axis=1)
    y_pred_class = np.argmax(y_pred, axis=1)
    return {
        "accuracy": accuracy_score(y_true_class, y_pred_class),
        "f1": f1_score(y_true_class, y_pred_class, average="weighted"),
        "mcc": matthews_corrcoef(y_true_class, y_pred_class)
    }

# src/train_mlp.py
import numpy as np
from config import CONFIG
from data_loader import load_data
from model import initialize_weights, forward_propagation
from backpropagation import compute_loss, backward_propagation, update_weights
from metrics import evaluate

if __name__ == "__main__":
    X_train, X_test, y_train, y_test = load_data("data/data.csv", "data/data_labels.csv")

    layer_dims = [X_train.shape[1]] + CONFIG["hidden_layers"] + [y_train.shape[1]]
    activations = [CONFIG["activation"]] * len(CONFIG["hidden_layers"]) + ["logistic"]
    weights = initialize_weights(layer_dims)
    v = {k: np.zeros_like(v) for k, v in weights.items()}

    for epoch in range(CONFIG["epochs"]):
        perm = np.random.permutation(X_train.shape[0])
        for i in range(0, X_train.shape[0], CONFIG["batch_size"]):
            idx = perm[i:i + CONFIG["batch_size"]]
            X_batch, y_batch = X_train[idx], y_train[idx]

            y_hat, cache = forward_propagation(X_batch, weights, activations)
            loss = compute_loss(y_hat, y_batch)
            grads = backward_propagation(X_batch, y_batch, weights, cache, activations)
            weights, v = update_weights(weights, grads, CONFIG["lr"], CONFIG["momentum"], v)

        if (epoch + 1) % 10 == 0:
            y_pred_train, _ = forward_propagation(X_train, weights, activations)
            train_metrics = evaluate(y_train, y_pred_train)
            print(f"Epoch {epoch+1}: Train Acc={train_metrics['accuracy']:.4f} F1={train_metrics['f1']:.4f} MCC={train_metrics['mcc']:.4f}")
